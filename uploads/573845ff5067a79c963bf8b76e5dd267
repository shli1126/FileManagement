#!/usr/bin/env python
# coding: utf-8

# In[118]:


import gzip
import time 
from collections import defaultdict


# In[119]:


def readGz(path):
  for l in gzip.open(path, 'rt'):
    yield eval(l)


# In[120]:


def readCSV(path):
  f = gzip.open(path, 'rt')
  f.readline()
  for l in f:
    yield l.strip().split(',')


# In[121]:


dataset = []
for l in readCSV("train_Interactions.csv.gz"):
    dataset.append(l)


# In[122]:


dataset[0]


# In[123]:


len(dataset)


# In[124]:


usersPerItem = defaultdict(set) 
itemsPerUser = defaultdict(set)

for d in dataset:
    user,item = d[0], d[1]
    usersPerItem[d[1]].add(d[0])
    itemsPerUser[d[0]].add(d[1])


# In[125]:


def Jaccard(s1, s2):
    numer = len(s1.intersection(s2))
    denom = len(s1.union(s2))
    if denom == 0:
        return 0
    return numer / denom


# In[126]:


len(itemsPerUser)


# In[127]:


def mostSimilar(i, N):
    similarities = []
    items = itemsPerUser[i]
    for i2 in itemsPerUser:
        if i2 == i: continue
        sim = Jaccard(items, itemsPerUser[i2])
        #sim = Pearson(i, i2) # Could use alternate similarity metrics straightforwardly
        similarities.append((sim,i2))
    similarities.sort(reverse=True)
    return similarities[:N]


# In[128]:


query = dataset[7][0]


# In[129]:


mostSimilar(query, 5)


# In[130]:


type(itemsPerUser['u28240108'])


# In[131]:


def predictRead(user,item, N):
    sim_user = []
    for u in mostSimilar(user, N):
        sim_user.append(u[1])

    sim_user_books = []
    for k in sim_user:
        books = list(itemsPerUser[k])
        for i in books:
            if i not in sim_user_books:
                sim_user_books.append(i)
 
    if item in sim_user_books:
        return 1
    else:
        return 0


# In[132]:


count = 0
test = []
while count < 100:
    test.append(predictRead(dataset[count][0],dataset[count][1], 3))
    count+=1
    


# In[133]:


start = time.time()

predictions = open("predictions_Read.csv", 'w')
for l in open("pairs_Read.csv"):
    if l.startswith("userID"):
    #header
        predictions.write(l)
        continue
    u,b = l.strip().split(',')
    
    if predictRead(u,b, 5) == 1:
        predictions.write(u + ',' + b + ",1\n")
    else:
        predictions.write(u + ',' + b + ",0\n")

predictions.close()

end = time.time()
time = end - start


# In[134]:


time


# In[136]:


time / 60


# In[61]:


### Would-read baseline: just rank which books are popular and which are not, and return '1' if a book is among the top-ranked

bookCount = defaultdict(int)
totalRead = 0

for user,book,_ in readCSV("train_Interactions.csv.gz"):
  bookCount[book] += 1
  totalRead += 1

mostPopular = [(bookCount[x], x) for x in bookCount]
mostPopular.sort()
mostPopular.reverse()

return1 = set()
count = 0
for ic, i in mostPopular:
  count += ic
  return1.add(i)
  if count > totalRead/2: break

predictions = open("predictions_Read.csv", 'w')
for l in open("pairs_Read.csv"):
  if l.startswith("userID"):
    #header
    predictions.write(l)
    continue
  u,b = l.strip().split(',')
  if b in return1:
    predictions.write(u + ',' + b + ",1\n")
  else:
    predictions.write(u + ',' + b + ",0\n")

predictions.close()


# In[44]:


### Category prediction baseline: Just consider some of the most common words from each category

catDict = {
  "children": 0,
  "comics_graphic": 1,
  "fantasy_paranormal": 2,
  "mystery_thriller_crime": 3,
  "young_adult": 4
}

predictions = open("predictions_Category.csv", 'w')
predictions.write("userID,reviewID,prediction\n")
for l in readGz("test_Category.json.gz"):
  cat = catDict['fantasy_paranormal'] # If there's no evidence, just choose the most common category in the dataset
  words = l['review_text'].lower()
  if 'children' in words:
    cat = catDict['children']
  if 'comic' in words:
    cat = catDict['comics_graphic']
  if 'fantasy' in words:
    cat = catDict['fantasy_paranormal']
  if 'mystery' in words:
    cat = catDict['mystery_thriller_crime']
  if 'love' in words:
    cat = catDict['young_adult']
  predictions.write(l['user_id'] + ',' + l['review_id'] + "," + str(cat) + "\n")

predictions.close()


# In[ ]:




